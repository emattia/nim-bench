{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install matplotlib seaborn numpy pandas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import requests  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_rgba\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import os\n",
    "import subprocess\n",
    "from rich.jupyter import print\n",
    "from rich.prompt import Prompt\n",
    "from constants import OUTDATA_DIR\n",
    "\n",
    "REQUEST_COLOR = \"#8E6A1A\"\n",
    "CONCURRENCY_COLOR = \"#37795D\"\n",
    "BG_COLOR = \"#F5EFF9\"\n",
    "LABELPAD=8\n",
    "\n",
    "custom_params = {\n",
    "    \"axes.spines.left\": False, \n",
    "    \"axes.spines.right\": False, \n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.bottom\": False,\n",
    "    \"axes.grid\": False,\n",
    "    \"axes.facecolor\": BG_COLOR,\n",
    "    \"figure.facecolor\": BG_COLOR\n",
    "}\n",
    "sns.set_theme(rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py:129\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3217\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3215\u001b[0m \u001b[38;5;66;03m# Store raw and processed history\u001b[39;00m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_history:\n\u001b[0;32m-> 3217\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m   3219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(cell, raw_cell)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/history.py:773\u001b[0m, in \u001b[0;36mHistoryManager.store_inputs\u001b[0;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_parsed\u001b[38;5;241m.\u001b[39mappend(source)\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_raw\u001b[38;5;241m.\u001b[39mappend(source_raw)\n\u001b[0;32m--> 773\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache_lock:\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_input_cache\u001b[38;5;241m.\u001b[39mappend((line_num, source, source_raw))\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;66;03m# Trigger to flush cache and write to DB.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_mapping_function(a, b, _max = 250, _min = 10):\n",
    "    def mapping_function(x):\n",
    "        if x < a or x > b:\n",
    "            raise ValueError(f\"The input number {x} is out of the range [{a}, {b}]\")\n",
    "        return _min + (x - a) * (_max - _min) / (b - a)\n",
    "    return mapping_function\n",
    "\n",
    "def create_alpha_colormap(hex_color, alpha_bottom, alpha_top):\n",
    "    rgb_color = to_rgba(hex_color)[:3] \n",
    "    return LinearSegmentedColormap.from_list(\n",
    "        'alpha_colormap',\n",
    "        [(rgb_color[0], rgb_color[1], rgb_color[2], alpha_bottom),\n",
    "         (rgb_color[0], rgb_color[1], rgb_color[2], alpha_top)]\n",
    "    )\n",
    "\n",
    "def format_func(value, tick_number):\n",
    "    dt = datetime.fromtimestamp(value)\n",
    "    return dt.strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path_candidates = os.listdir(OUTDATA_DIR)\n",
    "default_path = [os.path.join(OUTDATA_DIR, p) for p in default_path_candidates if p.endswith('_results.csv')][0]\n",
    "# data_path = Prompt.ask(\"Which data are you analyzing?\", default=default_path)\n",
    "data_path=default_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df = df[df.prompt_tokens>45589] # some tiny experiments in the raw data.\n",
    "df['total_tokens'] = df.prompt_tokens + df.completion_tokens\n",
    "def f(ts):\n",
    "    return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "df.start_ts = df.start_ts.apply(f)\n",
    "df.end_ts = df.end_ts.apply(f)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    check=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Split the output into lines (one line per GPU)\n",
    "gpu_names = result.stdout.strip().split('\\n')\n",
    "\n",
    "if len(gpu_names) > 1:\n",
    "    device = gpu_names[0]\n",
    "    print('Found multiple device types. Set to %, manually change it if you ran the experiment on a different device.' % device)\n",
    "else:\n",
    "    device = gpu_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_device = device.lower().replace(' ', '_')\n",
    "formatted_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any', inplace=True)\n",
    "p_tokens, c_tokens = df.prompt_tokens.sum(), df.completion_tokens.sum()\n",
    "total_gpu_time = str(timedelta(seconds=df.total_time.sum()))\n",
    "title = f\"{p_tokens:,} prompt tokens, {c_tokens:,} completion tokens, in {total_gpu_time} on {device}\"\n",
    "print(title + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['completion tokens / second'] = df.completion_tokens / df.total_time\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(9, 5))\n",
    "\n",
    "alpha_bottom = .2\n",
    "alpha_top = .8\n",
    "max_token_alpha_map = create_mapping_function(\n",
    "    df.max_tokens_per_request.min(), df.max_tokens_per_request.max(), \n",
    "    _max = alpha_top, _min = alpha_bottom\n",
    ")\n",
    "max_tokens_alphas = [max_token_alpha_map(v) for v in df.max_tokens_per_request]\n",
    "request_map_for_colorbar = create_alpha_colormap(REQUEST_COLOR, alpha_bottom, alpha_top)\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=request_map_for_colorbar, norm=plt.Normalize(vmin=alpha_bottom, vmax=alpha_top))\n",
    "sm.set_array([])\n",
    "\n",
    "for v in df.max_tokens_per_request.unique():\n",
    "    _df = df[df.max_tokens_per_request==v]\n",
    "    a = max_token_alpha_map(v)\n",
    "    ax.scatter(\n",
    "        _df['completion tokens / second'], _df['concurrent_requests'], \n",
    "        color = REQUEST_COLOR, \n",
    "        alpha = a,\n",
    "        s = _df.total_time // 3,\n",
    "    )\n",
    "    spline = UnivariateSpline(_df['completion tokens / second'], _df['concurrent_requests'], s=1)\n",
    "    _y = spline(_df['completion tokens / second'])\n",
    "    ax.plot(\n",
    "        _df['completion tokens / second'],\n",
    "        _y, \n",
    "        alpha = a, \n",
    "        color = REQUEST_COLOR,\n",
    "    )\n",
    "\n",
    "request_ticks = df.concurrent_requests.unique()\n",
    "divider = make_axes_locatable(ax)\n",
    "x_start = df['completion tokens / second'].min()\n",
    "x_dist = df['completion tokens / second'].max() - df['completion tokens / time'].min()\n",
    "factor = request_ticks.max() * .35\n",
    "y_start = request_ticks.max() + factor\n",
    "y_dist = factor // 7\n",
    "cax = ax.inset_axes([x_start, y_start, x_dist, y_dist], transform=ax.transData)\n",
    "cbar = fig.colorbar(sm, orientation='horizontal', cax=cax)\n",
    "cbar.set_ticks(max_tokens_alphas)\n",
    "cbar.set_ticklabels([str(int(max_tpr)) for max_tpr in df.max_tokens_per_request], color=REQUEST_COLOR)\n",
    "cbar.set_label('Max tokens per request', color=REQUEST_COLOR)\n",
    "\n",
    "specific_concurrency_ticks = df.concurrent_requests.unique()\n",
    "specific_concurrency_tick_labels = [str(int(tick)) for tick in specific_concurrency_ticks]\n",
    "ax.set_yticks(specific_concurrency_ticks)\n",
    "ax.set_yticklabels(specific_concurrency_tick_labels)\n",
    "ax.set_ylabel('Concurrency', labelpad=LABELPAD)\n",
    "ax.set_xlabel('Completion tokens / second', labelpad=LABELPAD)\n",
    "\n",
    "_10pct = df['completion tokens / second'].max() * .1\n",
    "x_min =  df['completion tokens / second'].min() - _10pct\n",
    "x_max = df['completion tokens / second'].max() + _10pct\n",
    "ax.set_xlim([x_min, x_max])\n",
    "_10pct = df.concurrent_requests.max() * .1\n",
    "y_min =  df.concurrent_requests.min() - _10pct\n",
    "y_max = df.concurrent_requests.max() + _10pct\n",
    "ax.set_ylim([y_min, y_max])\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "formatted_device = device.lower().replace(' ', '_')\n",
    "fig.savefig(f'concurrency_{formatted_device}_{now}.png')\n",
    "fig.savefig(f'concurrency_{formatted_device}_{now}_transparent.png', transparent=True)\n",
    "\n",
    "ax.text(15, 15, 'Circle size is proportional to \\nbillable GPU server time.', color=REQUEST_COLOR)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_token = 0.75\n",
    "words_in_a_page = 500\n",
    "seconds_per_hour = 3600\n",
    "\n",
    "print(f\"(Estimated) number of document pages processed is {round(p_tokens * words_per_token * (1 / words_in_a_page))}.\")\n",
    "print(f\"(Estimated) number of summary pages generated is {round(c_tokens * words_per_token * (1 / words_in_a_page))}.\")\n",
    "print(f\"Billable compute time is {round(df.total_time.sum() / 3600, 3)} {device} hours.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplot_mosaic(mosaic=[\n",
    "    [ 'time and tokens - prompt' ],\n",
    "    [ 'time and tokens - completion'] \n",
    "], figsize=(10,8))\n",
    "\n",
    "alpha_bottom = .2\n",
    "alpha_top = 1.\n",
    "max_token_alpha_map = create_mapping_function(\n",
    "    df.max_tokens_per_request.min(), df.max_tokens_per_request.max(), \n",
    "    _max = alpha_top, _min = alpha_bottom\n",
    ")\n",
    "max_tokens_alphas = [max_token_alpha_map(v) for v in df.max_tokens_per_request]\n",
    "request_map_for_colorbar = create_alpha_colormap(REQUEST_COLOR, alpha_bottom, alpha_top)\n",
    "sm = plt.cm.ScalarMappable(cmap=request_map_for_colorbar, norm=plt.Normalize(vmin=alpha_bottom, vmax=alpha_top))\n",
    "sm.set_array([])\n",
    "\n",
    "max_prompt_tokens = df.prompt_tokens.max()\n",
    "min_prompt_tokens = df.prompt_tokens.min()\n",
    "prompt_map = create_mapping_function(a=min_prompt_tokens, b=max_prompt_tokens)\n",
    "prompt_tokens_size_scaling = [prompt_map(p_tokens) for p_tokens in df.prompt_tokens]\n",
    "prompt_tokens_size_scaling_labels = [\n",
    "    f\"{p:,}\"\n",
    "    for p in [min_prompt_tokens, max_prompt_tokens]\n",
    "]\n",
    "\n",
    "max_completion_tokens = df.completion_tokens.max()\n",
    "min_completion_tokens = df.completion_tokens.min()\n",
    "completions_map = create_mapping_function(a=min_completion_tokens, b=max_completion_tokens)\n",
    "completion_tokens_size_scaling = [completions_map(c_tokens) for c_tokens in df.completion_tokens]\n",
    "completion_tokens_size_scaling_labels = [\n",
    "    f\"{c:,}\"\n",
    "    for c in [min_completion_tokens, max_completion_tokens]\n",
    "]\n",
    "\n",
    "request_ticks = df.total_requests.unique()\n",
    "specific_request_tick_labels = [str(int(tick)) for tick in request_ticks]\n",
    "specific_concurrency_ticks = df.concurrent_requests.unique()\n",
    "specific_concurrency_tick_labels = [str(int(tick)) for tick in specific_concurrency_ticks]\n",
    "\n",
    "def color_ax(handle='time and tokens - prompt', is_prompt=True, is_bottom=False):\n",
    "\n",
    "    # Cbar only if bottom\n",
    "    if is_bottom:\n",
    "        ax[handle].set_xlabel(f'Total GPU time (s) in batch', labelpad=LABELPAD)\n",
    "        ax[handle].tick_params(axis='x', rotation=16,  size=14)\n",
    "        ax[handle].xaxis.set_major_formatter(FuncFormatter(lambda x, _: format_func(x, _)))\n",
    "    else:\n",
    "        ax[handle].set_title(title, y=1.3)\n",
    "        ax[handle].set_xticks([])\n",
    "        divider = make_axes_locatable(ax[handle])\n",
    "        x_start = df.total_time.min()\n",
    "        x_dist = df.total_time.max() - df.total_time.min()\n",
    "        factor = request_ticks.max() * .35\n",
    "        y_start = request_ticks.max() + factor\n",
    "        y_dist = factor // 7\n",
    "    \n",
    "        cax = ax[handle].inset_axes([x_start, y_start, x_dist, y_dist], transform=ax[handle].transData)\n",
    "        cbar = fig.colorbar(sm, orientation='horizontal', cax=cax)\n",
    "        cbar.set_ticks(max_tokens_alphas)\n",
    "        cbar.set_ticklabels([str(int(max_tpr)) for max_tpr in df.max_tokens_per_request], color=REQUEST_COLOR)\n",
    "        cbar.set_label('Max tokens per request', color=REQUEST_COLOR)\n",
    "        \n",
    "    # ax.tick_params(axis='x', rotation=70)\n",
    "\n",
    "    # ax[handle].yaxis.label.set_color(REQUEST_COLOR)\n",
    "    ax[handle].tick_params(axis='y', colors=REQUEST_COLOR)\n",
    "    ax[handle].set_ylabel('Total requests in batch', labelpad=LABELPAD)\n",
    "    ax[handle].set_yticks(request_ticks)\n",
    "    ax[handle].set_yticklabels(specific_request_tick_labels)\n",
    "\n",
    "    ax_time_tokens_twin = ax[handle].twinx()\n",
    "    ax_time_tokens_twin.set_ylabel('Number of threads')\n",
    "    # ax_time_tokens_twin.yaxis.label.set_color(CONCURRENCY_COLOR)\n",
    "    ax_time_tokens_twin.tick_params(axis='y', colors=CONCURRENCY_COLOR)\n",
    "    ax_time_tokens_twin.set_yticks(specific_concurrency_ticks)\n",
    "    ax_time_tokens_twin.set_yticklabels(specific_concurrency_tick_labels, color=CONCURRENCY_COLOR)\n",
    "\n",
    "    if is_prompt:\n",
    "        for t, r, a, s in zip(\n",
    "            df.total_time, df.total_requests, max_tokens_alphas, prompt_tokens_size_scaling\n",
    "        ):\n",
    "            ax[handle].scatter(\n",
    "                t, \n",
    "                r, \n",
    "                marker='o',\n",
    "                color = REQUEST_COLOR, \n",
    "                s = s,\n",
    "                alpha = a\n",
    "            )\n",
    "    else:\n",
    "        for t, r, a, s in zip(\n",
    "            df.total_time, df.total_requests, max_tokens_alphas, completion_tokens_size_scaling\n",
    "        ):\n",
    "            ax[handle].scatter(\n",
    "                t, \n",
    "                r, \n",
    "                marker='s',\n",
    "                color = REQUEST_COLOR, \n",
    "                s = s,\n",
    "                alpha = a\n",
    "            )\n",
    "        \n",
    "    \n",
    "    for val in df.max_tokens_per_request.unique():\n",
    "        _df = df[df.max_tokens_per_request == val]\n",
    "        a = max_token_alpha_map(val)\n",
    "        slope, intercept = np.polyfit(_df.total_time, _df.total_requests, deg=1)\n",
    "        _y = _df.total_time * slope + intercept\n",
    "        ax[handle].plot(\n",
    "            _df.total_time, \n",
    "            _y, \n",
    "            alpha = a, \n",
    "            color = REQUEST_COLOR,\n",
    "        )\n",
    "        x_pos = _df.total_time.values[-1]\n",
    "        x_pos -= 0.05 * request_ticks.max()\n",
    "        y_pos = _y.values[-1]\n",
    "        y_pos *= 1.05\n",
    "        ax[handle].text(x_pos, y_pos, f\"Slope = {round(slope, 1)}\", fontsize=12, color=REQUEST_COLOR, alpha=a)\n",
    "\n",
    "    if is_prompt:\n",
    "        handles = [\n",
    "            ax[handle].scatter([], [], s=prompt_map(min_prompt_tokens), color=REQUEST_COLOR, marker='o'),\n",
    "            ax[handle].scatter([], [], s=prompt_map(max_prompt_tokens), color=REQUEST_COLOR, marker='o')\n",
    "        ]\n",
    "        leg = ax[handle].legend(\n",
    "            handles, \n",
    "            prompt_tokens_size_scaling_labels, \n",
    "            title=\"Prompt tokens in batch\",\n",
    "            bbox_to_anchor=(.95, 0.45),\n",
    "            borderpad=0.8\n",
    "        )\n",
    "    else:\n",
    "        handles = [\n",
    "            ax[handle].scatter([], [], s=completions_map(min_completion_tokens), color=REQUEST_COLOR, marker='s'),\n",
    "            ax[handle].scatter([], [], s=completions_map(max_completion_tokens), color=REQUEST_COLOR, marker='s')\n",
    "        ]\n",
    "        leg = ax[handle].legend(\n",
    "            handles, \n",
    "            completion_tokens_size_scaling_labels, \n",
    "            title=\"Completion tokens in batch\",\n",
    "            bbox_to_anchor=(.95, 0.45),\n",
    "            borderpad=0.8\n",
    "        )\n",
    "    \n",
    "    leg._legend_box.align = \"left\"\n",
    "    \n",
    "    _10pct = request_ticks.max() * .1\n",
    "    ax[handle].set_ylim([request_ticks.min() - _10pct, request_ticks.max() + _10pct])\n",
    "    \n",
    "    for val in specific_concurrency_ticks:\n",
    "        ax_time_tokens_twin.axhline(y=val, alpha=0.1, color=CONCURRENCY_COLOR)\n",
    "    \n",
    "    _10pct = specific_concurrency_ticks.max() * .1\n",
    "    ax_time_tokens_twin.set_ylim(\n",
    "        [specific_concurrency_ticks.min() - _10pct, specific_concurrency_ticks.max() + _10pct]\n",
    "    )\n",
    "\n",
    "color_ax(handle='time and tokens - prompt', is_prompt=True, is_bottom=False)\n",
    "color_ax(handle='time and tokens - completion', is_prompt=False, is_bottom=True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "formatted_device = device.lower().replace(' ', '_')\n",
    "fig.savefig(f'results_{formatted_device}_{now}.png')\n",
    "fig.savefig(f'results_{formatted_device}_{now}_transparent.png', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promotheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMETHEUS_URL = 'http://localhost:9090/'\n",
    "START_TIME = datetime.utcnow() - timedelta(hours=9) \n",
    "END_TIME = datetime.utcnow()\n",
    "STEP = '15s'                         # Query step size\n",
    "\n",
    "QUERIES = [\n",
    "    'num_requests_running',\n",
    "    'gpu_cache_usage_perc',\n",
    "    'num_requests_waiting',\n",
    "    'generation_tokens_total',\n",
    "    'prompt_tokens_total',\n",
    "    'request_prompt_tokens_sum',\n",
    "    'time_to_first_token_seconds_sum',\n",
    "]\n",
    "\n",
    "def to_unix_timestamp(dt):\n",
    "    return int(dt.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prometheus_to_dataframe(result, metric_name):\n",
    "    if 'data' in result and 'result' in result['data']:\n",
    "        records = []\n",
    "        for metric in result['data']['result']:\n",
    "            metric_labels = metric['metric']\n",
    "            for value in metric['values']:\n",
    "                timestamp = datetime.fromtimestamp(value[0])\n",
    "                value = float(value[1])\n",
    "                record = {'timestamp': timestamp, 'value': value, 'metric': metric_name}\n",
    "                record.update(metric_labels)\n",
    "                records.append(record)\n",
    "        return pd.DataFrame(records)\n",
    "    else:\n",
    "        print(f\"No data found for query: {metric_name}\")\n",
    "        raise ValueError('Could not make pandas df from prometheus metrics.')\n",
    "        # return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_prometheus(query, start_time=None, end_time=None, step=None):\n",
    "    url = f\"{PROMETHEUS_URL}/api/v1/query\"\n",
    "    params = {'query': query}\n",
    "\n",
    "    if start_time and end_time and step:\n",
    "        url = f\"{PROMETHEUS_URL}/api/v1/query_range\"\n",
    "        params.update({\n",
    "            'start': to_unix_timestamp(start_time),\n",
    "            'end': to_unix_timestamp(end_time),\n",
    "            'step': step\n",
    "        })\n",
    "        \n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Query failed with status code {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataframes = []\n",
    "for query in QUERIES:\n",
    "    result = query_prometheus(query, start_time=START_TIME, end_time=END_TIME, step=STEP)\n",
    "    _df = prometheus_to_dataframe(result, query)\n",
    "    all_dataframes.append(_df)\n",
    "combined_df = pd.concat(all_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(combined_df.metric.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_smi_out = pd.read_csv(\n",
    "    '%s/%s_gpu_util.csv' % ( OUTDATA_DIR, formatted_device)\n",
    ")\n",
    "nv_smi_out.Timestamp = nv_smi_out.Timestamp.apply(lambda x: datetime.fromtimestamp(x))\n",
    "nv_smi_out.rename(columns={'Timestamp': 'timestamp'}, inplace=True)\n",
    "# gpu_util = nv_smi_out.rename(columns={'GPU Utilization (%)': 'value'}).drop(columns=['GPU core utilization (%)'])\n",
    "# gpu_util['metric'] = ['GPU core utilization (%)'] * gpu_util.shape[0]\n",
    "# mem_util = nv_smi_out.rename(columns={'GPU Memory Utilization (%)': 'value'}).drop(columns=['GPU memory utilization (%)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "def overlay_metric(full_df, ax, metric_type, label, a, c):\n",
    "    metric_df = full_df[full_df.metric==metric_type]\n",
    "    metric_df.plot(x='timestamp', y='value', label=label, ax=ax, alpha=a, color=c)\n",
    "    ax.tick_params(axis='y', colors=c) \n",
    "\n",
    "overlay_metric(combined_df, ax, metric_type='gpu_cache_usage_perc', label='K/V cache utilization', a=0.4, c=CONCURRENCY_COLOR)\n",
    "ax.legend(loc=2)\n",
    "kv_cache_util = [label.get_text() for label in ax.get_yticklabels()]\n",
    "kv_cache_util = [tick for tick in kv_cache_util if tick]  # Filter out empty strings\n",
    "kv_cache_util = [float(tick.replace('−', '-')) if '−' in tick else float(tick) for tick in kv_cache_util]\n",
    "kv_cache_util = [str(round(tick * 100)) + '%' for tick in kv_cache_util]\n",
    "ax.set_yticklabels(kv_cache_util)\n",
    "\n",
    "overlay_metric(nv_smi_out, ax, metric_type='GPU Utilization (%)', label='GPU core utilization (%)', a=0.6, c='r')\n",
    "overlay_metric(nv_smi_out, ax, metric_type='GPU Utilization (%)', label='GPU core utilization (%)', a=0.6, c='r')\n",
    "\n",
    "twin_ax=ax.twinx()\n",
    "overlay_metric(combined_df, twin_ax, metric_type='num_requests_running', label='Number running requests', a=0.7, c=REQUEST_COLOR)\n",
    "specific_concurrency_ticks = df.concurrent_requests.unique()\n",
    "specific_concurrency_tick_labels = [str(round(tick * 100, 2)) for tick in specific_concurrency_ticks]\n",
    "twin_ax.set_yticks(specific_concurrency_ticks)\n",
    "twin_ax.set_yticklabels(specific_concurrency_tick_labels, color=REQUEST_COLOR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(kv_cache_util, dtype=np.int16) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in kv_cache_util:\n",
    "    print(type(v), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* More tokens per request --> More "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
